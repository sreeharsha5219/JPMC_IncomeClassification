{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income Classification & Segmentation — Walkthrough\n",
    "\n",
    "Author: Sree Harsha Koyi  ",
    "\n",
    "This notebook walks line-by-line through the core steps the project implements: data loading, preprocessing, model training/evaluation, threshold trade-offs, and segmentation. It mirrors the code in `src/` and the artifacts under `reports/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment & Imports\n",
    "\n",
    "If needed, uncomment the `pip install` cell to install dependencies in a fresh environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q numpy pandas scikit-learn matplotlib seaborn xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            ConfusionMatrixDisplay, confusion_matrix)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Make project package importable\n",
    "ROOT = Path.cwd().resolve()\n",
    "if (ROOT / 'src').exists():\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from src.data_loader import load_census_data, split_features_target_weight, infer_feature_types\n",
    "from src.preprocessing import build_preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the CPS dataset using the provided headers. We keep raw tokens at load time and handle coercion during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'census-bureau.data'\n",
    "COLS_PATH = 'census-bureau.columns'\n",
    "SEED = 17\n",
    "\n",
    "raw = load_census_data(DATA_PATH, COLS_PATH)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Features/Target/Weights & Infer Types\n",
    "\n",
    "We exclude `label` and `weight` from features; `weight` is used as `sample_weight` for training and metrics." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all, w_all = split_features_target_weight(raw)\n",
    "num_cols, cat_cols = infer_feature_types(X_all)\n",
    "len(X_all), len(num_cols), len(cat_cols), num_cols[:5], cat_cols[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Exploratory Data Analysis (weighted)\n",
    "\n",
    "We summarize target balance, missingness, and selected numeric/categorical distributions with survey weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution (weighted)\n",
    "def weighted_counts(y, w):\n",
    "    df = pd.DataFrame({'y': y, 'w': w})\n",
    "    agg = df.groupby('y')['w'].sum().rename('weighted').reset_index()\n",
    "    agg['share'] = agg['weighted'] / agg['weighted'].sum()\n",
    "    return agg\n",
    "\n",
    "target_w = weighted_counts(y_all, w_all)\n",
    "target_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness (treat '?' and empty as missing)\n",
    "def missingness_table(df, w):\n",
    "    rows = []\n",
    "    total = len(df)\n",
    "    total_w = float(w.sum()) if float(w.sum()) > 0 else np.nan\n",
    "    for col in df.columns:\n",
    "        miss = (df[col] == '?') | (df[col].isna()) | (df[col] == '')\n",
    "        mc = int(miss.sum())\n",
    "        mw = float(w[miss].sum())\n",
    "        rows.append({\n",
    "            'column': col,\n",
    "            'missing_count': mc,\n",
    "            'missing_pct': mc / total if total > 0 else np.nan,\n",
    "            'missing_weight': mw,\n",
    "            'missing_weight_pct': (mw / total_w) if total_w and total_w > 0 else np.nan\n",
    "        })\n",
    "    res = pd.DataFrame(rows).sort_values('missing_weight', ascending=False).reset_index(drop=True)\n",
    "    return res\n",
    "\n",
    "miss_tbl = missingness_table(X_all, w_all)\n",
    "miss_tbl.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summaries (weighted)\n",
    "def wmean(x, w):\n",
    "    x = pd.to_numeric(x, errors='coerce')\n",
    "    return float(np.average(x.fillna(0), weights=w.reindex_like(x).fillna(0)))\n",
    "\n",
    "def wstd(x, w):\n",
    "    x = pd.to_numeric(x, errors='coerce')\n",
    "    wv = w.reindex_like(x).fillna(0).values\n",
    "    xv = x.fillna(0).values\n",
    "    mu = np.average(xv, weights=wv)\n",
    "    var = np.average((xv - mu)**2, weights=wv)\n",
    "    return float(np.sqrt(var))\n",
    "\n",
    "def wquant(x, w, q):\n",
    "    x = pd.to_numeric(x, errors='coerce')\n",
    "    m = x.notna() & w.notna()\n",
    "    xv, wv = x[m].values, w[m].values\n",
    "    if len(xv) == 0: return np.nan\n",
    "    order = np.argsort(xv)\n",
    "    xv, wv = xv[order], wv[order]\n",
    "    cw = np.cumsum(wv)\n",
    "    t = q * cw[-1]\n",
    "    idx = np.searchsorted(cw, t, side='left')\n",
    "    idx = np.clip(idx, 0, len(xv)-1)\n",
    "    return float(xv[idx])\n",
    "\n",
    "rows = []\n",
    "for col in num_cols:\n",
    "    s = X_all[col]\n",
    "    rows.append({\n",
    "        'column': col,\n",
    "        'w_mean': wmean(s, w_all),\n",
    "        'w_std': wstd(s, w_all),\n",
    "        'w_p10': wquant(s, w_all, 0.10),\n",
    "        'w_p50': wquant(s, w_all, 0.50),\n",
    "        'w_p90': wquant(s, w_all, 0.90),\n",
    "    })\n",
    "pd.DataFrame(rows).sort_values('column').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical top categories (weighted) for a few key fields\n",
    "key_cats = [\n",
    "    'education', 'marital stat', 'major occupation code', 'major industry code', 'race', 'sex', 'citizenship'\n",
    "]\n",
    "rows = []\n",
    "dfc = X_all.copy()\n",
    "dfc['__w'] = w_all.values\n",
    "for col in key_cats:\n",
    "    if col in dfc.columns:\n",
    "        g = dfc.groupby(col)['__w'].sum().sort_values(ascending=False).head(10)\n",
    "        for k, v in g.items():\n",
    "            rows.append({'column': col, 'category': str(k), 'weighted_count': float(v)})\n",
    "pd.DataFrame(rows).sort_values(['column','weighted_count'], ascending=[True, False]).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipeline\n",
    "\n",
    "Numeric: coerce→median impute→scale. Categorical: '?'→NA→most‑frequent impute→one‑hot (ignore unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = build_preprocessor(num_cols, cat_cols)\n",
    "pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X_all, y_all, w_all, test_size=0.2, random_state=SEED, stratify=y_all\n",
    ")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Models\n",
    "\n",
    "We fit two compact baselines: Logistic Regression and XGBoost (hist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_eval(pipe, name):\n",
    "    pipe.fit(X_train, y_train, clf__sample_weight=w_train)\n",
    "    proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    out = {\n",
    "        'model': name,\n",
    "        'roc_auc': float(roc_auc_score(y_test, proba, sample_weight=w_test)),\n",
    "        'accuracy': float(accuracy_score(y_test, pred, sample_weight=w_test)),\n",
    "        'precision': float(precision_score(y_test, pred, zero_division=0, sample_weight=w_test)),\n",
    "        'recall': float(recall_score(y_test, pred, zero_division=0, sample_weight=w_test)),\n",
    "        'f1': float(f1_score(y_test, pred, zero_division=0, sample_weight=w_test)),\n",
    "    }\n",
    "    return out, proba\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=SEED)\n",
    "pipe_lr = Pipeline([('pre', pre), ('clf', lr)])\n",
    "res_lr, proba_lr = fit_eval(pipe_lr, 'logistic_regression')\n",
    "res_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.1,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    objective='binary:logistic', eval_metric='auc', tree_method='hist',\n",
    "    n_jobs=-1, random_state=SEED\n",
    ")\n",
    "pipe_xgb = Pipeline([('pre', pre), ('clf', xgb)])\n",
    "res_xgb, proba_xgb = fit_eval(pipe_xgb, 'xgboost_hist')\n",
    "res_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plots: ROC and Confusion Matrix (Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(proba, y_true, title='ROC Curve'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, proba, sample_weight=w_test)\n",
    "    auc = roc_auc_score(y_true, proba, sample_weight=w_test)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "    plt.plot([0,1],[0,1],'--',c='gray')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(title); plt.legend(); plt.show()\n",
    "\n",
    "def plot_confusion(pred, y_true, title='Confusion Matrix (weighted)'):\n",
    "    cm = confusion_matrix(y_true, pred, sample_weight=w_test)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    disp.plot(ax=ax, values_format='.0f'); plt.title(title); plt.show()\n",
    "\n",
    "# Use XGBoost results for plots\n",
    "pred_xgb = (proba_xgb >= 0.5).astype(int)\n",
    "plot_roc(proba_xgb, y_test, title='ROC — XGBoost (test)')\n",
    "plot_confusion(pred_xgb, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold–Metrics Sweep\n",
    "\n",
    "Choose operating points by business cost and goals (recall vs precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_table(proba, y_true, w=None, steps=51):\n",
    "    th = np.linspace(0,1,steps)\n",
    "    rows = []\n",
    "    for t in th:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        rows.append({\n",
    "            'threshold': float(t),\n",
    "            'precision': float(precision_score(y_true, pred, zero_division=0, sample_weight=w)),\n",
    "            'recall': float(recall_score(y_true, pred, zero_division=0, sample_weight=w)),\n",
    "            'f1': float(f1_score(y_true, pred, zero_division=0, sample_weight=w)),\n",
    "            'accuracy': float(accuracy_score(y_true, pred, sample_weight=w)),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "tab = threshold_table(proba_xgb, y_test, w_test, steps=41)\n",
    "tab.sort_values('f1', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Segmentation (KMeans)\n",
    "\n",
    "Use the same preprocessed feature space to build 6 clusters and profile them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Preprocess to numeric array\n",
    "Z = Pipeline([('pre', pre)]).fit_transform(X_all)\n",
    "kmeans = KMeans(n_clusters=6, random_state=SEED, n_init=10)\n",
    "cl = kmeans.fit_predict(Z)\n",
    "sil = silhouette_score(Z, cl)\n",
    "sil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Artifacts (Optional)\n",
    "\n",
    "Mirror the script outputs if you want to persist results from the notebook run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "out_reports = Path('reports'); out_reports.mkdir(parents=True, exist_ok=True)\n",
    "out_plots = out_reports / 'plots'; out_plots.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Example: save model\n",
    "dump(pipe_xgb, 'models/classifier.joblib')\n",
    "\n",
    "# Example: save threshold table\n",
    "tab.to_csv(out_reports / 'threshold_metrics_from_notebook.csv', index=False)\n",
    "\n",
    "# Example: save cluster assignments\n",
    "pd.DataFrame({'cluster': cl}).to_csv('outputs/segments_from_notebook.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Notes\n",
    "- The notebook mirrors the project scripts: `src/train_classifier.py` and `src/segment.py`.\n",
    "- For production, prefer running the scripts for deterministic artifacts and versioning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
